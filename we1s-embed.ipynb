{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "import re\n",
    "import json\n",
    "from zipfile import ZipFile\n",
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en') # Installed with python -m spacy download en\n",
    "\n",
    "# Switch on logging for gensim model training\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 300\n",
    "VOCAB_MAX = 10000\n",
    "EPOCHS = 3\n",
    "MODEL_NAME = '10K_300_txtfiles_newspapers.model' # If model exists it is loaded\n",
    "USE_SPACY = False\n",
    "USE_LEMMAS = False # Only valid with USE_SPACY = True\n",
    "USE_ENTITIES = False # Only valid with USE_SPACY = True\n",
    "REMOVE_PUNCTUATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_archive_sources(pathfiles_dir, archives_dir):\n",
    "    archives = []\n",
    "    for pathfile_fname in os.listdir(pathfiles_dir):\n",
    "        if pathfile_fname.endswith('.txt'):\n",
    "            with open(pathfiles_dir + '/' + pathfile_fname) as pathfile:\n",
    "                for line in pathfile.readlines():\n",
    "                    # Strip trailing spaces, single quotes, comma, and newline\n",
    "                    line_stripped = re.findall(r\"'(.*?)'\", line, re.DOTALL)\n",
    "                    if line_stripped: # Strip empty lines\n",
    "                        assert len(line_stripped) == 1\n",
    "                        archive_fname = archives_dir + '/' + line_stripped[0]\n",
    "                        if os.path.isfile(archive_fname):\n",
    "                            archives += [archive_fname]\n",
    "                        else:\n",
    "                            raise FileExistsError('The following data file does not exist: ' + archive_fname)\n",
    "    return archives\n",
    "    \n",
    "    \n",
    "def load_from_txtfile_sources(txtfiles_dir):\n",
    "    txtfiles = []\n",
    "    for txtfile_fname in os.listdir(txtfiles_dir):\n",
    "        if txtfile_fname.endswith('.txt'):\n",
    "            txtfiles.append(txtfiles_dir + '/' + txtfile_fname)\n",
    "    return txtfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter download hack: run !tar chvfz ntext_files_clean.tar.gz text_files_clean in new notebook\n",
    "\n",
    "# pathfiles_dir = 'corpora/filepaths'\n",
    "# archives_dir = 'corpora/data-new'\n",
    "# archives = load_archive_sources(pathfiles_dir, archives_dir)\n",
    "\n",
    "# Files from 20181105_1452_us-newspapers-humanities-250-dedupe\n",
    "txtfiles_dir = 'corpora/text_files_clean_newspapers_250'\n",
    "txtfiles = load_from_txtfile_sources(txtfiles_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentences(object):\n",
    "    def __init__(self, files, remove_punctuation=True, use_spacy=False, use_lemmas=False, use_entities=False):\n",
    "        self.files = files\n",
    "        # Unfortunately SpaCy is really slow when called for every text, so make it optional for testing\n",
    "        self.use_spacy = use_spacy\n",
    "        self.use_lemmas = use_lemmas\n",
    "        self.use_entities = use_entities\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self._punctuation_regex = re.compile('[%s]' % re.escape(punctuation))\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in self.files:\n",
    "            if fname.endswith('.zip'):\n",
    "                # WE1S JSOn archive\n",
    "                with ZipFile(fname, 'r') as archive:\n",
    "                    for json_fname in archive.namelist():\n",
    "                        if not json_fname.startswith('README'): # Exclude weird empty README files in archives\n",
    "                            lines = archive.open(json_fname).readlines()\n",
    "                            json_ = ''.join([line.decode('UTF-8') for line in lines])\n",
    "                            text = json.loads(json_)['content']\n",
    "                            for sentence in self.yield_sentences_from_text(text):\n",
    "                                yield(sentence)\n",
    "            elif fname.endswith('.txt'):\n",
    "                # Mallet plain text\n",
    "                with open(fname, 'r') as txtfile:\n",
    "                    lines = txtfile.readlines()\n",
    "                    text = ''.join(lines)\n",
    "                    for sentence in self.yield_sentences_from_text(text):\n",
    "                        yield(sentence)\n",
    "            else:\n",
    "                raise Exception('File seems to be neither Mallet plain text file or WE1S JSON archive.')\n",
    "    \n",
    "    def yield_sentences_from_text(self, text):\n",
    "        if self.use_spacy:\n",
    "            doc = nlp(text)\n",
    "\n",
    "            # Detect and merge entitites\n",
    "            if (self.use_entities):\n",
    "                for ent in doc.ents:\n",
    "                    ent.merge(tag=ent.root.tag_, lemma=ent.text, ent_type=ent.root.ent_type_)\n",
    "\n",
    "            # Detect sentences\n",
    "            for sentence in doc.sents: \n",
    "                words = []\n",
    "                # Detect tokens\n",
    "                for token in sentence:\n",
    "                    # Add lowercase token to list if it is not punctuation or whitespace\n",
    "                    if ((not token.is_punct) or (not self.remove_punctuation)) and (not token.is_space):\n",
    "                        if (self.use_lemmas):\n",
    "                            if token.lemma_ == '-PRON-':\n",
    "                                word = token.text\n",
    "                            else:\n",
    "                                word = token.lemma_\n",
    "                        else:\n",
    "                            word = token.text\n",
    "                        words.append(word.lower())\n",
    "                yield words\n",
    "\n",
    "        else:\n",
    "            for sentence in text.split('.'):\n",
    "                words = []\n",
    "                if self.remove_punctuation:\n",
    "                    sentence = self._punctuation_regex.sub('', sentence)\n",
    "                sentence = sentence.lower()\n",
    "                for word in sentence.split():\n",
    "                    word = word.strip()\n",
    "                    words.append(word)\n",
    "                yield words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Sentences(txtfiles, \n",
    "                use_spacy=USE_SPACY, \n",
    "                use_lemmas=USE_LEMMAS, \n",
    "                use_entities=USE_ENTITIES, \n",
    "                remove_punctuation=REMOVE_PUNCTUATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-07 06:34:28,717 : INFO : collecting all words and their counts\n",
      "2018-11-07 06:34:28,718 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-11-07 06:34:28,764 : INFO : PROGRESS: at sentence #10000, processed 122455 words, keeping 15962 word types\n",
      "2018-11-07 06:34:28,810 : INFO : PROGRESS: at sentence #20000, processed 244529 words, keeping 23554 word types\n",
      "2018-11-07 06:34:28,857 : INFO : PROGRESS: at sentence #30000, processed 366648 words, keeping 30501 word types\n",
      "2018-11-07 06:34:28,905 : INFO : PROGRESS: at sentence #40000, processed 495772 words, keeping 35257 word types\n",
      "2018-11-07 06:34:28,950 : INFO : PROGRESS: at sentence #50000, processed 615408 words, keeping 39687 word types\n",
      "2018-11-07 06:34:28,994 : INFO : PROGRESS: at sentence #60000, processed 731235 words, keeping 43895 word types\n",
      "2018-11-07 06:34:29,037 : INFO : PROGRESS: at sentence #70000, processed 844077 words, keeping 47198 word types\n",
      "2018-11-07 06:34:29,085 : INFO : PROGRESS: at sentence #80000, processed 972345 words, keeping 50824 word types\n",
      "2018-11-07 06:34:29,133 : INFO : PROGRESS: at sentence #90000, processed 1100502 words, keeping 54061 word types\n",
      "2018-11-07 06:34:29,181 : INFO : PROGRESS: at sentence #100000, processed 1226565 words, keeping 57327 word types\n",
      "2018-11-07 06:34:29,228 : INFO : PROGRESS: at sentence #110000, processed 1354005 words, keeping 60324 word types\n",
      "2018-11-07 06:34:29,257 : INFO : PROGRESS: at sentence #120000, processed 1424341 words, keeping 61678 word types\n",
      "2018-11-07 06:34:29,306 : INFO : PROGRESS: at sentence #130000, processed 1553497 words, keeping 64403 word types\n",
      "2018-11-07 06:34:29,354 : INFO : PROGRESS: at sentence #140000, processed 1678842 words, keeping 67180 word types\n",
      "2018-11-07 06:34:29,405 : INFO : PROGRESS: at sentence #150000, processed 1817527 words, keeping 69818 word types\n",
      "2018-11-07 06:34:29,450 : INFO : PROGRESS: at sentence #160000, processed 1933941 words, keeping 72226 word types\n",
      "2018-11-07 06:34:29,499 : INFO : PROGRESS: at sentence #170000, processed 2066506 words, keeping 74529 word types\n",
      "2018-11-07 06:34:29,545 : INFO : PROGRESS: at sentence #180000, processed 2189026 words, keeping 77120 word types\n",
      "2018-11-07 06:34:29,592 : INFO : PROGRESS: at sentence #190000, processed 2315833 words, keeping 79170 word types\n",
      "2018-11-07 06:34:29,632 : INFO : PROGRESS: at sentence #200000, processed 2416458 words, keeping 81420 word types\n",
      "2018-11-07 06:34:29,678 : INFO : PROGRESS: at sentence #210000, processed 2538382 words, keeping 83366 word types\n",
      "2018-11-07 06:34:29,725 : INFO : PROGRESS: at sentence #220000, processed 2661445 words, keeping 85440 word types\n",
      "2018-11-07 06:34:29,773 : INFO : PROGRESS: at sentence #230000, processed 2782650 words, keeping 87444 word types\n",
      "2018-11-07 06:34:29,819 : INFO : PROGRESS: at sentence #240000, processed 2903803 words, keeping 89426 word types\n",
      "2018-11-07 06:34:29,869 : INFO : PROGRESS: at sentence #250000, processed 3032479 words, keeping 91381 word types\n",
      "2018-11-07 06:34:29,927 : INFO : PROGRESS: at sentence #260000, processed 3172920 words, keeping 93449 word types\n",
      "2018-11-07 06:34:29,977 : INFO : PROGRESS: at sentence #270000, processed 3302173 words, keeping 94976 word types\n",
      "2018-11-07 06:34:30,036 : INFO : PROGRESS: at sentence #280000, processed 3426510 words, keeping 96728 word types\n",
      "2018-11-07 06:34:30,088 : INFO : PROGRESS: at sentence #290000, processed 3556208 words, keeping 98377 word types\n",
      "2018-11-07 06:34:30,135 : INFO : PROGRESS: at sentence #300000, processed 3674363 words, keeping 100321 word types\n",
      "2018-11-07 06:34:30,185 : INFO : PROGRESS: at sentence #310000, processed 3792993 words, keeping 102103 word types\n",
      "2018-11-07 06:34:30,234 : INFO : PROGRESS: at sentence #320000, processed 3918570 words, keeping 104194 word types\n",
      "2018-11-07 06:34:30,289 : INFO : PROGRESS: at sentence #330000, processed 4050768 words, keeping 105994 word types\n",
      "2018-11-07 06:34:30,335 : INFO : PROGRESS: at sentence #340000, processed 4165879 words, keeping 107542 word types\n",
      "2018-11-07 06:34:30,386 : INFO : PROGRESS: at sentence #350000, processed 4296833 words, keeping 109064 word types\n",
      "2018-11-07 06:34:30,440 : INFO : PROGRESS: at sentence #360000, processed 4431170 words, keeping 110561 word types\n",
      "2018-11-07 06:34:30,491 : INFO : PROGRESS: at sentence #370000, processed 4559156 words, keeping 111982 word types\n",
      "2018-11-07 06:34:30,537 : INFO : PROGRESS: at sentence #380000, processed 4669719 words, keeping 113603 word types\n",
      "2018-11-07 06:34:30,583 : INFO : PROGRESS: at sentence #390000, processed 4783106 words, keeping 115359 word types\n",
      "2018-11-07 06:34:30,625 : INFO : PROGRESS: at sentence #400000, processed 4885297 words, keeping 116519 word types\n",
      "2018-11-07 06:34:30,689 : INFO : PROGRESS: at sentence #410000, processed 5012465 words, keeping 118100 word types\n",
      "2018-11-07 06:34:30,740 : INFO : PROGRESS: at sentence #420000, processed 5141650 words, keeping 119739 word types\n",
      "2018-11-07 06:34:30,795 : INFO : PROGRESS: at sentence #430000, processed 5276753 words, keeping 121707 word types\n",
      "2018-11-07 06:34:30,845 : INFO : PROGRESS: at sentence #440000, processed 5403874 words, keeping 123001 word types\n",
      "2018-11-07 06:34:30,894 : INFO : PROGRESS: at sentence #450000, processed 5523143 words, keeping 124881 word types\n",
      "2018-11-07 06:34:30,944 : INFO : PROGRESS: at sentence #460000, processed 5649788 words, keeping 126861 word types\n",
      "2018-11-07 06:34:30,988 : INFO : PROGRESS: at sentence #470000, processed 5757310 words, keeping 128267 word types\n",
      "2018-11-07 06:34:31,039 : INFO : PROGRESS: at sentence #480000, processed 5881473 words, keeping 129840 word types\n",
      "2018-11-07 06:34:31,088 : INFO : PROGRESS: at sentence #490000, processed 6007817 words, keeping 131229 word types\n",
      "2018-11-07 06:34:31,140 : INFO : PROGRESS: at sentence #500000, processed 6129329 words, keeping 132524 word types\n",
      "2018-11-07 06:34:31,185 : INFO : PROGRESS: at sentence #510000, processed 6244524 words, keeping 133783 word types\n",
      "2018-11-07 06:34:31,237 : INFO : PROGRESS: at sentence #520000, processed 6354669 words, keeping 134941 word types\n",
      "2018-11-07 06:34:31,292 : INFO : PROGRESS: at sentence #530000, processed 6483240 words, keeping 136584 word types\n",
      "2018-11-07 06:34:31,344 : INFO : PROGRESS: at sentence #540000, processed 6615217 words, keeping 137916 word types\n",
      "2018-11-07 06:34:31,397 : INFO : PROGRESS: at sentence #550000, processed 6740474 words, keeping 138959 word types\n",
      "2018-11-07 06:34:31,454 : INFO : PROGRESS: at sentence #560000, processed 6872166 words, keeping 140163 word types\n",
      "2018-11-07 06:34:31,502 : INFO : PROGRESS: at sentence #570000, processed 6991671 words, keeping 141483 word types\n",
      "2018-11-07 06:34:31,553 : INFO : PROGRESS: at sentence #580000, processed 7114756 words, keeping 143052 word types\n",
      "2018-11-07 06:34:31,607 : INFO : PROGRESS: at sentence #590000, processed 7254683 words, keeping 144355 word types\n",
      "2018-11-07 06:34:31,658 : INFO : PROGRESS: at sentence #600000, processed 7376304 words, keeping 145655 word types\n",
      "2018-11-07 06:34:31,710 : INFO : PROGRESS: at sentence #610000, processed 7511078 words, keeping 146958 word types\n",
      "2018-11-07 06:34:31,760 : INFO : PROGRESS: at sentence #620000, processed 7627745 words, keeping 148429 word types\n",
      "2018-11-07 06:34:31,809 : INFO : PROGRESS: at sentence #630000, processed 7754035 words, keeping 149502 word types\n",
      "2018-11-07 06:34:31,861 : INFO : PROGRESS: at sentence #640000, processed 7880960 words, keeping 150580 word types\n",
      "2018-11-07 06:34:31,915 : INFO : PROGRESS: at sentence #650000, processed 8008896 words, keeping 151782 word types\n",
      "2018-11-07 06:34:31,967 : INFO : PROGRESS: at sentence #660000, processed 8144093 words, keeping 152922 word types\n",
      "2018-11-07 06:34:32,023 : INFO : PROGRESS: at sentence #670000, processed 8281244 words, keeping 154041 word types\n",
      "2018-11-07 06:34:32,072 : INFO : PROGRESS: at sentence #680000, processed 8395357 words, keeping 155065 word types\n",
      "2018-11-07 06:34:32,126 : INFO : PROGRESS: at sentence #690000, processed 8534274 words, keeping 156208 word types\n",
      "2018-11-07 06:34:32,181 : INFO : PROGRESS: at sentence #700000, processed 8665584 words, keeping 157327 word types\n",
      "2018-11-07 06:34:32,235 : INFO : PROGRESS: at sentence #710000, processed 8793588 words, keeping 158499 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-07 06:34:32,290 : INFO : PROGRESS: at sentence #720000, processed 8935701 words, keeping 159844 word types\n",
      "2018-11-07 06:34:32,347 : INFO : PROGRESS: at sentence #730000, processed 9064296 words, keeping 160843 word types\n",
      "2018-11-07 06:34:32,390 : INFO : PROGRESS: at sentence #740000, processed 9171784 words, keeping 162123 word types\n",
      "2018-11-07 06:34:32,444 : INFO : PROGRESS: at sentence #750000, processed 9302676 words, keeping 163320 word types\n",
      "2018-11-07 06:34:32,493 : INFO : PROGRESS: at sentence #760000, processed 9424167 words, keeping 164447 word types\n",
      "2018-11-07 06:34:32,537 : INFO : PROGRESS: at sentence #770000, processed 9533797 words, keeping 165671 word types\n",
      "2018-11-07 06:34:32,590 : INFO : PROGRESS: at sentence #780000, processed 9661286 words, keeping 166688 word types\n",
      "2018-11-07 06:34:32,642 : INFO : PROGRESS: at sentence #790000, processed 9796239 words, keeping 167984 word types\n",
      "2018-11-07 06:34:32,691 : INFO : PROGRESS: at sentence #800000, processed 9921207 words, keeping 169014 word types\n",
      "2018-11-07 06:34:32,743 : INFO : PROGRESS: at sentence #810000, processed 10042674 words, keeping 170336 word types\n",
      "2018-11-07 06:34:32,777 : INFO : PROGRESS: at sentence #820000, processed 10124514 words, keeping 171087 word types\n",
      "2018-11-07 06:34:32,833 : INFO : PROGRESS: at sentence #830000, processed 10259779 words, keeping 172233 word types\n",
      "2018-11-07 06:34:32,876 : INFO : PROGRESS: at sentence #840000, processed 10365894 words, keeping 173083 word types\n",
      "2018-11-07 06:34:32,897 : INFO : collected 173569 word types from a corpus of 10416851 raw words and 844048 sentences\n",
      "2018-11-07 06:34:32,939 : INFO : max_final_vocab=10000 and min_count=1 resulted in calc_min_count=71, effective_min_count=71\n",
      "2018-11-07 06:34:32,942 : INFO : Loading a fresh vocabulary\n",
      "2018-11-07 06:34:32,988 : INFO : effective_min_count=71 retains 9957 unique words (5% of original 173569, drops 163612)\n",
      "2018-11-07 06:34:32,989 : INFO : effective_min_count=71 leaves 9490920 word corpus (91% of original 10416851, drops 925931)\n",
      "2018-11-07 06:34:33,004 : INFO : deleting the raw counts dictionary of 173569 items\n",
      "2018-11-07 06:34:33,006 : INFO : sample=0.001 downsamples 41 most-common words\n",
      "2018-11-07 06:34:33,006 : INFO : downsampling leaves estimated 7279013 word corpus (76.7% of prior 9490920)\n",
      "2018-11-07 06:34:33,011 : INFO : constructing a huffman tree from 9957 words\n",
      "2018-11-07 06:34:33,186 : INFO : built huffman tree with maximum node depth 17\n",
      "2018-11-07 06:34:33,204 : INFO : estimated required memory for 9957 words and 300 dimensions: 42815100 bytes\n",
      "2018-11-07 06:34:33,205 : INFO : resetting layer weights\n",
      "2018-11-07 06:34:33,285 : INFO : training model with 8 workers on 9957 vocabulary and 300 features, using sg=1 hs=1 sample=0.001 negative=5 window=5\n",
      "2018-11-07 06:34:34,308 : INFO : EPOCH 1 - PROGRESS: at 4.26% examples, 301590 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:34:35,327 : INFO : EPOCH 1 - PROGRESS: at 8.74% examples, 308482 words/s, in_qsize 14, out_qsize 1\n",
      "2018-11-07 06:34:36,335 : INFO : EPOCH 1 - PROGRESS: at 13.85% examples, 316535 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:34:37,338 : INFO : EPOCH 1 - PROGRESS: at 18.08% examples, 318925 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:34:38,350 : INFO : EPOCH 1 - PROGRESS: at 22.68% examples, 321416 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:34:39,370 : INFO : EPOCH 1 - PROGRESS: at 27.47% examples, 322812 words/s, in_qsize 16, out_qsize 0\n",
      "2018-11-07 06:34:40,381 : INFO : EPOCH 1 - PROGRESS: at 31.67% examples, 322059 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:34:41,391 : INFO : EPOCH 1 - PROGRESS: at 36.21% examples, 323461 words/s, in_qsize 14, out_qsize 1\n",
      "2018-11-07 06:34:42,429 : INFO : EPOCH 1 - PROGRESS: at 40.90% examples, 324157 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:34:43,442 : INFO : EPOCH 1 - PROGRESS: at 45.83% examples, 326845 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:34:44,452 : INFO : EPOCH 1 - PROGRESS: at 51.09% examples, 331631 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:34:45,480 : INFO : EPOCH 1 - PROGRESS: at 56.39% examples, 334598 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:34:46,481 : INFO : EPOCH 1 - PROGRESS: at 61.80% examples, 337922 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:34:47,485 : INFO : EPOCH 1 - PROGRESS: at 66.68% examples, 340036 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:34:48,498 : INFO : EPOCH 1 - PROGRESS: at 71.65% examples, 342061 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:34:49,510 : INFO : EPOCH 1 - PROGRESS: at 76.68% examples, 343509 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:34:50,532 : INFO : EPOCH 1 - PROGRESS: at 81.63% examples, 345398 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:34:51,548 : INFO : EPOCH 1 - PROGRESS: at 86.53% examples, 347105 words/s, in_qsize 14, out_qsize 0\n",
      "2018-11-07 06:34:52,572 : INFO : EPOCH 1 - PROGRESS: at 91.87% examples, 347853 words/s, in_qsize 14, out_qsize 1\n",
      "2018-11-07 06:34:53,580 : INFO : EPOCH 1 - PROGRESS: at 97.37% examples, 349507 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:34:53,941 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-07 06:34:53,961 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-07 06:34:53,962 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-07 06:34:53,999 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-07 06:34:54,005 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-07 06:34:54,009 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-07 06:34:54,018 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-07 06:34:54,021 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-07 06:34:54,022 : INFO : EPOCH - 1 : training on 10416851 raw words (7279918 effective words) took 20.7s, 351171 effective words/s\n",
      "2018-11-07 06:34:55,036 : INFO : EPOCH 2 - PROGRESS: at 4.71% examples, 336627 words/s, in_qsize 14, out_qsize 1\n",
      "2018-11-07 06:34:56,064 : INFO : EPOCH 2 - PROGRESS: at 10.06% examples, 352094 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:34:57,077 : INFO : EPOCH 2 - PROGRESS: at 15.52% examples, 359038 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:34:58,081 : INFO : EPOCH 2 - PROGRESS: at 20.56% examples, 362800 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:34:59,091 : INFO : EPOCH 2 - PROGRESS: at 26.02% examples, 366281 words/s, in_qsize 16, out_qsize 0\n",
      "2018-11-07 06:35:00,120 : INFO : EPOCH 2 - PROGRESS: at 30.86% examples, 364066 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:01,125 : INFO : EPOCH 2 - PROGRESS: at 35.35% examples, 359975 words/s, in_qsize 16, out_qsize 0\n",
      "2018-11-07 06:35:02,186 : INFO : EPOCH 2 - PROGRESS: at 40.09% examples, 355069 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:03,209 : INFO : EPOCH 2 - PROGRESS: at 44.50% examples, 351904 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:04,211 : INFO : EPOCH 2 - PROGRESS: at 49.28% examples, 348815 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:05,257 : INFO : EPOCH 2 - PROGRESS: at 53.82% examples, 346818 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:06,263 : INFO : EPOCH 2 - PROGRESS: at 58.47% examples, 345673 words/s, in_qsize 14, out_qsize 1\n",
      "2018-11-07 06:35:07,280 : INFO : EPOCH 2 - PROGRESS: at 63.27% examples, 344447 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:08,291 : INFO : EPOCH 2 - PROGRESS: at 67.69% examples, 343518 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:09,294 : INFO : EPOCH 2 - PROGRESS: at 71.83% examples, 341462 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:10,306 : INFO : EPOCH 2 - PROGRESS: at 76.43% examples, 340807 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:11,307 : INFO : EPOCH 2 - PROGRESS: at 80.53% examples, 339229 words/s, in_qsize 15, out_qsize 1\n",
      "2018-11-07 06:35:12,309 : INFO : EPOCH 2 - PROGRESS: at 84.83% examples, 339228 words/s, in_qsize 13, out_qsize 1\n",
      "2018-11-07 06:35:13,314 : INFO : EPOCH 2 - PROGRESS: at 89.29% examples, 338526 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:14,316 : INFO : EPOCH 2 - PROGRESS: at 93.72% examples, 337669 words/s, in_qsize 13, out_qsize 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-07 06:35:15,334 : INFO : EPOCH 2 - PROGRESS: at 98.60% examples, 337249 words/s, in_qsize 13, out_qsize 0\n",
      "2018-11-07 06:35:15,423 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-07 06:35:15,429 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-07 06:35:15,463 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-07 06:35:15,493 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-07 06:35:15,515 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-07 06:35:15,518 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-07 06:35:15,519 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-07 06:35:15,527 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-07 06:35:15,527 : INFO : EPOCH - 2 : training on 10416851 raw words (7278223 effective words) took 21.5s, 338458 effective words/s\n",
      "2018-11-07 06:35:16,539 : INFO : EPOCH 3 - PROGRESS: at 4.51% examples, 324076 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:17,541 : INFO : EPOCH 3 - PROGRESS: at 9.88% examples, 350394 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:18,545 : INFO : EPOCH 3 - PROGRESS: at 15.35% examples, 358853 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:19,565 : INFO : EPOCH 3 - PROGRESS: at 20.15% examples, 357954 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:20,569 : INFO : EPOCH 3 - PROGRESS: at 25.71% examples, 364183 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:21,602 : INFO : EPOCH 3 - PROGRESS: at 30.84% examples, 365462 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:22,617 : INFO : EPOCH 3 - PROGRESS: at 35.95% examples, 366649 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:23,626 : INFO : EPOCH 3 - PROGRESS: at 41.18% examples, 368329 words/s, in_qsize 13, out_qsize 0\n",
      "2018-11-07 06:35:24,648 : INFO : EPOCH 3 - PROGRESS: at 46.30% examples, 367580 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:25,657 : INFO : EPOCH 3 - PROGRESS: at 51.43% examples, 368164 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:26,673 : INFO : EPOCH 3 - PROGRESS: at 56.89% examples, 369126 words/s, in_qsize 14, out_qsize 0\n",
      "2018-11-07 06:35:27,678 : INFO : EPOCH 3 - PROGRESS: at 62.17% examples, 369122 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:28,708 : INFO : EPOCH 3 - PROGRESS: at 67.27% examples, 369356 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:29,724 : INFO : EPOCH 3 - PROGRESS: at 72.27% examples, 369867 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:30,726 : INFO : EPOCH 3 - PROGRESS: at 77.27% examples, 369748 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:31,768 : INFO : EPOCH 3 - PROGRESS: at 82.24% examples, 369658 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:32,794 : INFO : EPOCH 3 - PROGRESS: at 87.37% examples, 369853 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:33,814 : INFO : EPOCH 3 - PROGRESS: at 92.67% examples, 370220 words/s, in_qsize 14, out_qsize 1\n",
      "2018-11-07 06:35:34,815 : INFO : EPOCH 3 - PROGRESS: at 98.05% examples, 370542 words/s, in_qsize 15, out_qsize 0\n",
      "2018-11-07 06:35:35,038 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-07 06:35:35,059 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-07 06:35:35,066 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-07 06:35:35,078 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-07 06:35:35,086 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-07 06:35:35,099 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-07 06:35:35,111 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-07 06:35:35,122 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-07 06:35:35,123 : INFO : EPOCH - 3 : training on 10416851 raw words (7279176 effective words) took 19.6s, 371513 effective words/s\n",
      "2018-11-07 06:35:35,123 : INFO : training on a 31250553 raw words (21837317 effective words) took 61.8s, 353141 effective words/s\n",
      "2018-11-07 06:35:35,123 : INFO : saving Word2Vec object under 10K_300_txtfiles_newspapers.model, separately None\n",
      "2018-11-07 06:35:35,124 : INFO : not storing attribute vectors_norm\n",
      "2018-11-07 06:35:35,124 : INFO : not storing attribute cum_table\n",
      "2018-11-07 06:35:35,423 : INFO : saved 10K_300_txtfiles_newspapers.model\n"
     ]
    }
   ],
   "source": [
    "# Load or train model\n",
    "model = None # Make sure we're not accidentally continuing training\n",
    "if os.path.isfile(MODEL_NAME):\n",
    "    model = model = Word2Vec.load(MODEL_NAME)\n",
    "else:\n",
    "    # https://radimrehurek.com/gensim/models/word2vec.html\n",
    "    # max_final_vocab needs gensim >3.5.0 and min_count=1 to work\n",
    "    # sg: use SkipGram\n",
    "    # hs: use hierarchical softmax\n",
    "    model = Word2Vec(gen, \n",
    "                     size=DIM, \n",
    "                     max_final_vocab=VOCAB_MAX, \n",
    "                     min_count=1, \n",
    "                     window=5, \n",
    "                     workers=8, \n",
    "                     iter=EPOCHS, \n",
    "                     sg=1, \n",
    "                     hs=1)\n",
    "    model.save(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9957\n"
     ]
    }
   ],
   "source": [
    "print(len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-07 06:35:35,441 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sciences', 0.5206330418586731), ('socialsciences', 0.4732058048248291), ('mathematics', 0.4585806131362915), ('arts', 0.4397183954715729), ('behavioral', 0.40036070346832275), ('science', 0.3988550901412964), ('rockefellerfoundation', 0.3805062174797058), ('philosophy', 0.3769809305667877), ('bioethics', 0.37596845626831055), ('polytechnic', 0.3609108328819275)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian/miniconda3/envs/py36-ai/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('humanities', topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between each possible pair of points in a cluster, \n",
    "# then average over total number of pairs\n",
    "def cluster_density(cluster):\n",
    "    total_similarity = 0\n",
    "    for a in cluster:\n",
    "        for b in cluster:\n",
    "            if a != b:\n",
    "                cosine_similarity = model.wv.n_similarity([a], [b])\n",
    "                total_similarity += cosine_similarity\n",
    "    average_similarity = total_similarity / (len(cluster)**2)\n",
    "    return average_similarity\n",
    "\n",
    "# Random entry from model vocabulary\n",
    "def random_vocab():\n",
    "    return random.choice(list(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load topic model from keys\n",
    "topics = []\n",
    "with open('corpora/keys.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        topic = line.split()\n",
    "        topics.append(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare density of topics to random topic\n",
    "for topic in topics:\n",
    "    try:\n",
    "        words = topic[3:] # First two are number and value\n",
    "        random_topic = [random_vocab() for i in range(len(words))]\n",
    "        density_topic = cluster_density(words)\n",
    "        density_random_topic = cluster_density(random_topic)\n",
    "        if density_topic < density_random_topic: # Only print when less dense than random topic\n",
    "            print(topic)\n",
    "            print(density_topic)\n",
    "            print(random_topic)\n",
    "            print(density_random_topic)\n",
    "    # Because we are limiting teh vocabulary, in very rare cases we might encounter an out-of-vocabulary word\n",
    "    # Usually the topic top-10 words should also be somehwhat common words and thus not be pruned\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14945123366505644 ['132', '0.02035', 'trump', 'president', 'white', 'house', 'center', 'protest', 'kennedy', 'charlottesville', 'protesters', 'year', 'american', 'friday', 'letter', 'rally', 'committee', 'members', 'week', 'humanities', 'event']\n",
      "0.16588607743198489 ['40', '0.11178', 'center', 'event', 'public', 'humanities', 'series', 'events', 'conference', 'program', 'information', 'discussion', 'lecture', 'day', 'held', 'talk', 'open', 'council', 'year', 'call', 'free']\n",
      "0.16706047476165825 ['164', '0.16792', 'officials', 'members', 'week', 'year', 'group', 'meeting', 'made', 'decision', 'humanities', 'asked', 'board', 'plan', 'month', 'called', 'director', 'letter', 'expected', 'announced', 'statement']\n",
      "0.17458591692977482 ['62', '0.06347', 'arts', 'art', 'cultural', 'artists', 'culture', 'music', 'nea', 'organizations', 'theater', 'community', 'creative', 'support', 'dance', 'performing', 'humanities', 'city', 'people', 'center', 'artistic']\n",
      "0.17609455501430749 ['74', '0.03046', 'university', 'father', 'york', 'graduated', 'mrs', 'mother', 'mr', 'college', 'received', 'daughter', 'son', 'humanities', 'school', 'married', 'degree', 'bride', 'master', 'retired', 'director']\n",
      "0.18159728411033196 ['73', '0.05944', 'york', 'city', 'brooklyn', 'manhattan', 'mayor', 'bronx', 'college', 'queens', 'street', 'columbia', 'island', 'yesterday', 'cuny', 'east', 'times', 'urban', 'humanities', 'hall', 'bloomberg']\n",
      "0.182204416061947 ['69', '0.02933', 'medal', 'national', 'humanities', 'arts', 'award', 'president', 'obama', 'won', 'awards', 'tony', 'year', 'musical', 'actor', 'broadway', 'music', 'honor', 'actress', 'contributions', 'prize']\n",
      "0.18554738817023642 ['158', '0.04361', 'education', 'curriculum', 'students', 'courses', 'college', 'humanities', 'liberal', 'arts', 'study', 'core', 'history', 'report', 'colleges', 'general', 'american', 'social', 'knowledge', 'western', 'requirements']\n",
      "0.20081777648747334 ['70', '0.04385', 'award', 'awards', 'year', 'prize', 'national', 'received', 'winners', 'winner', 'honor', 'humanities', 'won', 'community', 'academy', 'ceremony', 'competition', 'annual', 'years', 'association', 'contest']\n",
      "0.2014975197567248 ['75', '0.11852', 'professor', 'university', 'studies', 'history', 'scholars', 'american', 'humanities', 'study', 'academic', 'research', 'institute', 'social', 'center', 'harvard', 'field', 'literature', 'scholar', 'work', 'department']\n",
      "0.2128364564505992 ['219', '0.02529', 'literature', 'literary', 'bloom', 'english', 'criticism', 'shakespeare', 'books', 'critics', 'works', 'reading', 'texts', 'canon', 'humanities', 'book', 'theory', 'critic', 'text', 'read', 'century']\n",
      "0.2201289524220758 ['157', '0.05716', 'college', 'job', 'degree', 'graduates', 'education', 'jobs', 'major', 'students', 'career', 'majors', 'degrees', 'business', 'school', 'arts', 'engineering', 'liberal', 'humanities', 'graduate', 'fields']\n",
      "0.23170216018219053 ['242', '0.04831', 'philosophy', 'humanities', 'human', 'knowledge', 'philosopher', 'philosophers', 'philosophical', 'ideas', 'thinking', 'questions', 'world', 'moral', 'intellectual', 'truth', 'science', 'understanding', 'thought', 'scientific', 'social']\n",
      "0.33497094245696507 ['14', '0.03522', 'arts', 'nea', 'endowment', 'federal', 'grants', 'funding', 'endowments', 'government', 'neh', 'humanities', 'agency', 'support', 'money', 'congress', 'national', 'grant', 'private', 'funds', 'agencies']\n"
     ]
    }
   ],
   "source": [
    "# Order topics with \"humanities\" by density\n",
    "density_topic = {}\n",
    "for topic in topics:\n",
    "    try:\n",
    "        words = topic[3:]\n",
    "        if 'humanities' in words:\n",
    "            density = cluster_density(words)\n",
    "            density_topic[density] = topic\n",
    "    # Result of max. vocabulary limit\n",
    "    except KeyError:\n",
    "        pass\n",
    "for density in sorted(density_topic):\n",
    "    print(density, density_topic[density])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with matplotlib\n",
    "def plot_pyplot(vectors, words):\n",
    "    \n",
    "    print('Applying PCA')\n",
    "    vectors = PCA(n_components=100).fit_transform(vectors)\n",
    "    print('Applying T-SNE')\n",
    "    vectors = TSNE(n_components=2, learning_rate=100, perplexity=50).fit_transform(vectors)\n",
    "    \n",
    "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "    fig_size[0] = 100\n",
    "    fig_size[1] = 100\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "    def wordscatter(x, y, word, ax):\n",
    "        color = 'black'\n",
    "        if word in topicm100_4_noweights:\n",
    "            color = 'red' \n",
    "        ax.annotate(word, xy=(x, y), xytext=(x, y), color=color, alpha=0.4)\n",
    "        ax.update_datalim(np.column_stack([x, y]))\n",
    "        ax.autoscale()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    for i, word in enumerate(words):\n",
    "        wordscatter(vectors[i,0], vectors[i,1], word, ax=ax)\n",
    "    #ax.scatter(vectors[:,0], vectors[:,1])\n",
    "\n",
    "    plt.savefig('tsne.png', dpi = 100)\n",
    "\n",
    "# Prepare to be plotted with TensorBoard\n",
    "def plot_tb(vectors, words):\n",
    "    with open('data.tsv', 'w+') as f:\n",
    "        for vector in vectors.tolist():\n",
    "            for point in vector:\n",
    "                f.write(str(point) + '\\t')\n",
    "            f.write('\\n')\n",
    "\n",
    "    with open('metadata.tsv', 'w+') as f:\n",
    "        for word in words:\n",
    "            f.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate words and vectors\n",
    "words = []\n",
    "vectors = np.zeros((len(model.wv.vocab), DIM))\n",
    "for i, word in enumerate(model.wv.index2word):\n",
    "    vectors[i] = model.wv[word]\n",
    "    words.append(word)\n",
    "\n",
    "plot_tb(vectors, words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
