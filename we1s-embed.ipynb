{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "import re\n",
    "import json\n",
    "from zipfile import ZipFile\n",
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en') # Installed with python -m spacy download en\n",
    "\n",
    "# Switch on logging for gensim model training\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 300\n",
    "VOCAB_MAX = 10000\n",
    "EPOCHS = 3\n",
    "MODEL_NAME = '10K_300_txtfiles_newspapers.model' # If model exists it is loaded\n",
    "USE_SPACY = False\n",
    "USE_LEMMAS = False # Only valid with USE_SPACY = True\n",
    "USE_ENTITIES = False # Only valid with USE_SPACY = True\n",
    "REMOVE_PUNCTUATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_archive_sources(pathfiles_dir, archives_dir):\n",
    "    archives = []\n",
    "    for pathfile_fname in os.listdir(pathfiles_dir):\n",
    "        if pathfile_fname.endswith('.txt'):\n",
    "            with open(pathfiles_dir + '/' + pathfile_fname) as pathfile:\n",
    "                for line in pathfile.readlines():\n",
    "                    # Strip trailing spaces, single quotes, comma, and newline\n",
    "                    line_stripped = re.findall(r\"'(.*?)'\", line, re.DOTALL)\n",
    "                    if line_stripped: # Strip empty lines\n",
    "                        assert len(line_stripped) == 1\n",
    "                        archive_fname = archives_dir + '/' + line_stripped[0]\n",
    "                        if os.path.isfile(archive_fname):\n",
    "                            archives += [archive_fname]\n",
    "                        else:\n",
    "                            raise FileExistsError('The following data file does not exist: ' + archive_fname)\n",
    "    return archives\n",
    "    \n",
    "    \n",
    "def load_from_txtfile_sources(txtfiles_dir):\n",
    "    txtfiles = []\n",
    "    for txtfile_fname in os.listdir(txtfiles_dir):\n",
    "        if txtfile_fname.endswith('.txt'):\n",
    "            txtfiles.append(txtfiles_dir + '/' + txtfile_fname)\n",
    "    return txtfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter download hack: run !tar chvfz ntext_files_clean.tar.gz text_files_clean in new notebook\n",
    "\n",
    "# pathfiles_dir = 'corpora/filepaths'\n",
    "# archives_dir = 'corpora/data-new'\n",
    "# archives = load_archive_sources(pathfiles_dir, archives_dir)\n",
    "\n",
    "# Files from 20181105_1452_us-newspapers-humanities-250-dedupe\n",
    "txtfiles_dir = 'corpora/text_files_clean_newspapers_250'\n",
    "txtfiles = load_from_txtfile_sources(txtfiles_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentences(object):\n",
    "    def __init__(self, files, remove_punctuation=True, use_spacy=False, use_lemmas=False, use_entities=False):\n",
    "        self.files = files\n",
    "        # Unfortunately SpaCy is really slow when called for every text, so make it optional for testing\n",
    "        self.use_spacy = use_spacy\n",
    "        self.use_lemmas = use_lemmas\n",
    "        self.use_entities = use_entities\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self._punctuation_regex = re.compile('[%s]' % re.escape(punctuation))\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in self.files:\n",
    "            if fname.endswith('.zip'):\n",
    "                # WE1S JSOn archive\n",
    "                with ZipFile(fname, 'r') as archive:\n",
    "                    for json_fname in archive.namelist():\n",
    "                        if not json_fname.startswith('README'): # Exclude weird empty README files in archives\n",
    "                            lines = archive.open(json_fname).readlines()\n",
    "                            json_ = ''.join([line.decode('UTF-8') for line in lines])\n",
    "                            text = json.loads(json_)['content']\n",
    "                            for sentence in self.yield_sentences_from_text(text):\n",
    "                                yield(sentence)\n",
    "            elif fname.endswith('.txt'):\n",
    "                # Mallet plain text\n",
    "                with open(fname, 'r') as txtfile:\n",
    "                    lines = txtfile.readlines()\n",
    "                    text = ''.join(lines)\n",
    "                    for sentence in self.yield_sentences_from_text(text):\n",
    "                        yield(sentence)\n",
    "            else:\n",
    "                raise Exception('File seems to be neither Mallet plain text file or WE1S JSON archive.')\n",
    "    \n",
    "    def yield_sentences_from_text(self, text):\n",
    "        if self.use_spacy:\n",
    "            doc = nlp(text)\n",
    "\n",
    "            # Detect and merge entitites\n",
    "            if (self.use_entities):\n",
    "                for ent in doc.ents:\n",
    "                    ent.merge(tag=ent.root.tag_, lemma=ent.text, ent_type=ent.root.ent_type_)\n",
    "\n",
    "            # Detect sentences\n",
    "            for sentence in doc.sents: \n",
    "                words = []\n",
    "                # Detect tokens\n",
    "                for token in sentence:\n",
    "                    # Add lowercase token to list if it is not punctuation or whitespace\n",
    "                    if ((not token.is_punct) or (not self.remove_punctuation)) and (not token.is_space):\n",
    "                        if (self.use_lemmas):\n",
    "                            if token.lemma_ == '-PRON-':\n",
    "                                word = token.text\n",
    "                            else:\n",
    "                                word = token.lemma_\n",
    "                        else:\n",
    "                            word = token.text\n",
    "                        words.append(word.lower())\n",
    "                yield words\n",
    "\n",
    "        else:\n",
    "            for sentence in text.split('.'):\n",
    "                words = []\n",
    "                if self.remove_punctuation:\n",
    "                    sentence = self._punctuation_regex.sub('', sentence)\n",
    "                sentence = sentence.lower()\n",
    "                for word in sentence.split():\n",
    "                    word = word.strip()\n",
    "                    words.append(word)\n",
    "                yield words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Sentences(txtfiles, \n",
    "                use_spacy=USE_SPACY, \n",
    "                use_lemmas=USE_LEMMAS, \n",
    "                use_entities=USE_ENTITIES, \n",
    "                remove_punctuation=REMOVE_PUNCTUATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or train model\n",
    "model = None # Make sure we're not accidentally continuing training\n",
    "if os.path.isfile(MODEL_NAME):\n",
    "    model = model = Word2Vec.load(MODEL_NAME)\n",
    "else:\n",
    "    # https://radimrehurek.com/gensim/models/word2vec.html\n",
    "    # max_final_vocab needs gensim >3.5.0 and min_count=1 to work\n",
    "    # sg: use SkipGram\n",
    "    # hs: use hierarchical softmax\n",
    "    model = Word2Vec(gen, \n",
    "                     size=DIM, \n",
    "                     max_final_vocab=VOCAB_MAX, \n",
    "                     min_count=1, \n",
    "                     window=5, \n",
    "                     workers=8, \n",
    "                     iter=EPOCHS, \n",
    "                     sg=1, \n",
    "                     hs=1)\n",
    "    model.save(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9957\n"
     ]
    }
   ],
   "source": [
    "print(len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-07 06:35:35,441 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sciences', 0.5206330418586731), ('socialsciences', 0.4732058048248291), ('mathematics', 0.4585806131362915), ('arts', 0.4397183954715729), ('behavioral', 0.40036070346832275), ('science', 0.3988550901412964), ('rockefellerfoundation', 0.3805062174797058), ('philosophy', 0.3769809305667877), ('bioethics', 0.37596845626831055), ('polytechnic', 0.3609108328819275)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian/miniconda3/envs/py36-ai/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('humanities', topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between each possible pair of points in a cluster, \n",
    "# then average over total number of pairs\n",
    "def cluster_density(cluster):\n",
    "    total_similarity = 0\n",
    "    for a in cluster:\n",
    "        for b in cluster:\n",
    "            if a != b:\n",
    "                cosine_similarity = model.wv.n_similarity([a], [b])\n",
    "                total_similarity += cosine_similarity\n",
    "    average_similarity = total_similarity / (len(cluster)**2)\n",
    "    return average_similarity\n",
    "\n",
    "# Random entry from model vocabulary\n",
    "def random_vocab():\n",
    "    return random.choice(list(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load topic model from keys\n",
    "topics = []\n",
    "with open('corpora/keys.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        topic = line.split()\n",
    "        topics.append(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare density of topics to random topic\n",
    "for topic in topics:\n",
    "    try:\n",
    "        words = topic[3:] # First two are number and value\n",
    "        random_topic = [random_vocab() for i in range(len(words))]\n",
    "        density_topic = cluster_density(words)\n",
    "        density_random_topic = cluster_density(random_topic)\n",
    "        if density_topic < density_random_topic: # Only print when less dense than random topic\n",
    "            print(topic)\n",
    "            print(density_topic)\n",
    "            print(random_topic)\n",
    "            print(density_random_topic)\n",
    "    # Because we are limiting teh vocabulary, in very rare cases we might encounter an out-of-vocabulary word\n",
    "    # Usually the topic top-10 words should also be somehwhat common words and thus not be pruned\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14945123366505644 ['132', '0.02035', 'trump', 'president', 'white', 'house', 'center', 'protest', 'kennedy', 'charlottesville', 'protesters', 'year', 'american', 'friday', 'letter', 'rally', 'committee', 'members', 'week', 'humanities', 'event']\n",
      "0.16588607743198489 ['40', '0.11178', 'center', 'event', 'public', 'humanities', 'series', 'events', 'conference', 'program', 'information', 'discussion', 'lecture', 'day', 'held', 'talk', 'open', 'council', 'year', 'call', 'free']\n",
      "0.16706047476165825 ['164', '0.16792', 'officials', 'members', 'week', 'year', 'group', 'meeting', 'made', 'decision', 'humanities', 'asked', 'board', 'plan', 'month', 'called', 'director', 'letter', 'expected', 'announced', 'statement']\n",
      "0.17458591692977482 ['62', '0.06347', 'arts', 'art', 'cultural', 'artists', 'culture', 'music', 'nea', 'organizations', 'theater', 'community', 'creative', 'support', 'dance', 'performing', 'humanities', 'city', 'people', 'center', 'artistic']\n",
      "0.17609455501430749 ['74', '0.03046', 'university', 'father', 'york', 'graduated', 'mrs', 'mother', 'mr', 'college', 'received', 'daughter', 'son', 'humanities', 'school', 'married', 'degree', 'bride', 'master', 'retired', 'director']\n",
      "0.18159728411033196 ['73', '0.05944', 'york', 'city', 'brooklyn', 'manhattan', 'mayor', 'bronx', 'college', 'queens', 'street', 'columbia', 'island', 'yesterday', 'cuny', 'east', 'times', 'urban', 'humanities', 'hall', 'bloomberg']\n",
      "0.182204416061947 ['69', '0.02933', 'medal', 'national', 'humanities', 'arts', 'award', 'president', 'obama', 'won', 'awards', 'tony', 'year', 'musical', 'actor', 'broadway', 'music', 'honor', 'actress', 'contributions', 'prize']\n",
      "0.18554738817023642 ['158', '0.04361', 'education', 'curriculum', 'students', 'courses', 'college', 'humanities', 'liberal', 'arts', 'study', 'core', 'history', 'report', 'colleges', 'general', 'american', 'social', 'knowledge', 'western', 'requirements']\n",
      "0.20081777648747334 ['70', '0.04385', 'award', 'awards', 'year', 'prize', 'national', 'received', 'winners', 'winner', 'honor', 'humanities', 'won', 'community', 'academy', 'ceremony', 'competition', 'annual', 'years', 'association', 'contest']\n",
      "0.2014975197567248 ['75', '0.11852', 'professor', 'university', 'studies', 'history', 'scholars', 'american', 'humanities', 'study', 'academic', 'research', 'institute', 'social', 'center', 'harvard', 'field', 'literature', 'scholar', 'work', 'department']\n",
      "0.2128364564505992 ['219', '0.02529', 'literature', 'literary', 'bloom', 'english', 'criticism', 'shakespeare', 'books', 'critics', 'works', 'reading', 'texts', 'canon', 'humanities', 'book', 'theory', 'critic', 'text', 'read', 'century']\n",
      "0.2201289524220758 ['157', '0.05716', 'college', 'job', 'degree', 'graduates', 'education', 'jobs', 'major', 'students', 'career', 'majors', 'degrees', 'business', 'school', 'arts', 'engineering', 'liberal', 'humanities', 'graduate', 'fields']\n",
      "0.23170216018219053 ['242', '0.04831', 'philosophy', 'humanities', 'human', 'knowledge', 'philosopher', 'philosophers', 'philosophical', 'ideas', 'thinking', 'questions', 'world', 'moral', 'intellectual', 'truth', 'science', 'understanding', 'thought', 'scientific', 'social']\n",
      "0.33497094245696507 ['14', '0.03522', 'arts', 'nea', 'endowment', 'federal', 'grants', 'funding', 'endowments', 'government', 'neh', 'humanities', 'agency', 'support', 'money', 'congress', 'national', 'grant', 'private', 'funds', 'agencies']\n"
     ]
    }
   ],
   "source": [
    "# Order topics with \"humanities\" by density\n",
    "density_topic = {}\n",
    "for topic in topics:\n",
    "    try:\n",
    "        words = topic[3:]\n",
    "        if 'humanities' in words:\n",
    "            density = cluster_density(words)\n",
    "            density_topic[density] = topic\n",
    "    # Result of max. vocabulary limit\n",
    "    except KeyError:\n",
    "        pass\n",
    "for density in sorted(density_topic):\n",
    "    print(density, density_topic[density])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with matplotlib\n",
    "def plot_pyplot(vectors, words):\n",
    "    \n",
    "    print('Applying PCA')\n",
    "    vectors = PCA(n_components=100).fit_transform(vectors)\n",
    "    print('Applying T-SNE')\n",
    "    vectors = TSNE(n_components=2, learning_rate=100, perplexity=50).fit_transform(vectors)\n",
    "    \n",
    "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "    fig_size[0] = 100\n",
    "    fig_size[1] = 100\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "    def wordscatter(x, y, word, ax):\n",
    "        color = 'black'\n",
    "        if word in topicm100_4_noweights:\n",
    "            color = 'red' \n",
    "        ax.annotate(word, xy=(x, y), xytext=(x, y), color=color, alpha=0.4)\n",
    "        ax.update_datalim(np.column_stack([x, y]))\n",
    "        ax.autoscale()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    for i, word in enumerate(words):\n",
    "        wordscatter(vectors[i,0], vectors[i,1], word, ax=ax)\n",
    "    #ax.scatter(vectors[:,0], vectors[:,1])\n",
    "\n",
    "    plt.savefig('tsne.png', dpi = 100)\n",
    "\n",
    "# Prepare to be plotted with TensorBoard\n",
    "def plot_tb(vectors, words):\n",
    "    with open('data.tsv', 'w+') as f:\n",
    "        for vector in vectors.tolist():\n",
    "            for point in vector:\n",
    "                f.write(str(point) + '\\t')\n",
    "            f.write('\\n')\n",
    "\n",
    "    with open('metadata.tsv', 'w+') as f:\n",
    "        for word in words:\n",
    "            f.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate words and vectors\n",
    "words = []\n",
    "vectors = np.zeros((len(model.wv.vocab), DIM))\n",
    "for i, word in enumerate(model.wv.index2word):\n",
    "    vectors[i] = model.wv[word]\n",
    "    words.append(word)\n",
    "\n",
    "plot_tb(vectors, words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
